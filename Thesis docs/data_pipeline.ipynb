{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Art historians' network relations\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdflib\n",
    "import rdflib\n",
    "from rdflib import Namespace , Literal , URIRef\n",
    "from rdflib.namespace import RDF , RDFS\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "# utils\n",
    "import ssl, os.path, json, requests , ast\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "\n",
    "# spacy\n",
    "import spacy\n",
    "import random\n",
    "from spacy.util import minibatch, compounding\n",
    "from pathlib import Path\n",
    "\n",
    "# python3 -m spacy download en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# data proc \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from apyori import apriori\n",
    "\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, fpmax, fpgrowth\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "# graph data\n",
    "import networkx as nx\n",
    "from networkx.algorithms import bipartite\n",
    "\n",
    "# data viz\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# maps\n",
    "from ipywidgets import HTML\n",
    "from ipyleaflet import Map, Marker, Popup, LayersControl, AwesomeIcon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wikidata_reconciliation(query, q_class=None):\n",
    "    \"Find Wikidata QID for a query string\"\n",
    "    params = {\n",
    "        'action': 'wbsearchentities',\n",
    "        'format': 'json',\n",
    "        'language': 'en',\n",
    "        'search': query\n",
    "    }\n",
    "    # query wd API\n",
    "    API_WD = \"https://www.wikidata.org/w/api.php\"\n",
    "    r = requests.get(API_WD, params = params).json()\n",
    "\n",
    "    # double check if the entity belongs to the right class\n",
    "    if 'search' in r and len(r['search']) >= 1:\n",
    "        if q_class:\n",
    "            query_string = \"\"\"ASK {wd:\"\"\"+r['search'][0]['title']+\"\"\" a <https://www.wikidata.org/entity/\"\"\"+q_class+\"\"\">. }\"\"\"\n",
    "            res = return_sparql_query_results(query_string)\n",
    "            print(\"\\nRES\", query, query_string, res)\n",
    "            if res[\"boolean\"] == 'true':\n",
    "                return [ r['search'][0]['title'] , 'class_match']\n",
    "            else:\n",
    "                return [ r['search'][0]['title'] , 'no_class_match']\n",
    "        else:\n",
    "            return [ r['search'][0]['title'] , 'no_class_given']\n",
    "    else:\n",
    "        return 'not matched'\n",
    "    \n",
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the list of historians, names, and biographies from ARTchives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artists_query = \"\"\"\n",
    "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "    PREFIX wdt: <http://www.wikidata.org/wiki/Property:>\n",
    "    SELECT ?historian (sample(?names) as ?name) ?bio\n",
    "    WHERE { \n",
    "        ?collection wdt:P170 ?historian . \n",
    "        ?historian rdfs:label ?names. \n",
    "        OPTIONAL{?historian <http://purl.org/dc/terms/description> ?bio}\n",
    "    }\n",
    "    GROUP BY ?historian ?name ?bio\"\"\"\n",
    "\n",
    "art_historians = []\n",
    "\n",
    "try :\n",
    "    art_sparql = SPARQLWrapper(\"http://artchives.fondazionezeri.unibo.it/sparql\")\n",
    "    art_sparql.setQuery(artists_query)\n",
    "    art_sparql.setReturnFormat(JSON)\n",
    "    results = art_sparql.query().convert()\n",
    "    for result in results[\"results\"][\"bindings\"]:\n",
    "        historian = {}\n",
    "        historian[\"uri\"] = result[\"historian\"][\"value\"]\n",
    "        historian[\"name\"] = result[\"name\"][\"value\"]\n",
    "        if \"bio\" in result:\n",
    "            historian[\"bio\"] = result[\"bio\"][\"value\"]\n",
    "        art_historians.append(historian)\n",
    "except Exception as e:\n",
    "    print(e) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get historians' places of education and activity from Wikidata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "wikidata_endpoint = \"https://query.wikidata.org/bigdata/namespace/wdq/sparql\"\n",
    "\n",
    "if os.path.isfile(\"historian_places.json\"):\n",
    "    f = open('historian_places.json')\n",
    "    results = json.load(f)\n",
    "else:\n",
    "    historians_list = ' '.join(['<'+art_dict[\"uri\"]+'>' for art_dict in art_historians])\n",
    "    eduplace_query = \"\"\" \n",
    "    PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
    "    SELECT DISTINCT ?historian ?workplace ?workplace_label ?coordinates1 ?eduplace ?eduplace_label ?coordinates2\n",
    "    WHERE {\n",
    "            VALUES ?historian {\"\"\"+historians_list+\"\"\"} . \n",
    "            \n",
    "            optional {\n",
    "                ?historian wdt:P108 ?workplace . \n",
    "                ?workplace rdfs:label ?workplace_label ; \n",
    "                            wdt:P625 ?coordinates1; \n",
    "                            wdt:P31 ?type .\n",
    "                FILTER (langMatches(lang(?workplace_label), \"EN\")) \n",
    "                ?type rdfs:label ?type_label . \n",
    "                FILTER (langMatches(lang(?type_label), \"EN\"))\n",
    "                }\n",
    "            optional {\n",
    "                ?historian wdt:P69 ?eduplace . \n",
    "                ?eduplace rdfs:label ?eduplace_label ;\n",
    "                            wdt:P625 ?coordinates2; \n",
    "                            wdt:P31 ?type . \n",
    "                FILTER (langMatches(lang(?eduplace_label), \"EN\")) \n",
    "                ?type rdfs:label ?type_label . \n",
    "                FILTER (langMatches(lang(?type_label), \"EN\")) \n",
    "                }\n",
    "            } \n",
    "    GROUP BY ?historian ?workplace ?workplace_label ?coordinates1 ?eduplace ?eduplace_label ?coordinates2 \n",
    "    \"\"\"\n",
    "    try :\n",
    "        sparql_wd = SPARQLWrapper(wikidata_endpoint)\n",
    "        sparql_wd.setQuery(eduplace_query)\n",
    "        sparql_wd.setReturnFormat(JSON)\n",
    "        results = sparql_wd.query().convert()\n",
    "\n",
    "        #with open('historian_places.json', 'w') as f:\n",
    "        #    json.dump(results, f, indent=4)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Include results in a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty Graph\n",
    "g = rdflib.ConjunctiveGraph()\n",
    "\n",
    "# bind namespaces\n",
    "wd = Namespace(\"http://www.wikidata.org/entity/\") # remember that a prefix matches a URI until the last slash (or hashtag #)\n",
    "wdt = Namespace(\"http://www.wikidata.org/prop/direct/\")\n",
    "art = Namespace(\"https://w3id.org/artchives/\")\n",
    "rdfs = Namespace (\"http://www.w3.org/2000/01/rdf-schema#\")\n",
    "\n",
    "for result in results[\"results\"][\"bindings\"]:\n",
    "    historian_uri = result[\"historian\"][\"value\"]\n",
    "    name = [art_dict[\"name\"] for art_dict in art_historians if art_dict[\"uri\"] == historian_uri][0]\n",
    "    g.add(( URIRef(historian_uri) , RDFS.label , Literal(name) ))\n",
    "    if \"workplace\" in result: \n",
    "        workplace = result[\"workplace\"][\"value\"]\n",
    "        if \"workplace_label\" in result and \"coordinates1\" in result: \n",
    "            workplace_label = result[\"workplace_label\"][\"value\"]\n",
    "            work_coord = result[\"coordinates1\"][\"value\"][6:-1].split(\" \")\n",
    "            g.add(( URIRef(historian_uri) , URIRef(wdt.P108) , URIRef(workplace) ))\n",
    "            g.add(( URIRef(workplace) , RDFS.label , Literal(workplace_label) ))\n",
    "            g.add(( URIRef(workplace) , RDFS.comment , Literal(\"institution\") ))\n",
    "            g.add(( URIRef(workplace) , URIRef(wdt.P625) , Literal(work_coord) ))\n",
    "    if \"eduplace\" in result: \n",
    "        eduplace = result[\"eduplace\"][\"value\"]\n",
    "        if \"eduplace_label\" in result and \"coordinates2\" in result: \n",
    "            eduplace_label = result[\"eduplace_label\"][\"value\"]\n",
    "            eduplace_coord = result[\"coordinates2\"][\"value\"][6:-1].split(\" \")\n",
    "            g.add(( URIRef(historian_uri) , URIRef(wdt.P69) , URIRef(eduplace) ))\n",
    "            g.add(( URIRef(eduplace) , RDFS.label , Literal(eduplace_label) ))\n",
    "            g.add(( URIRef(eduplace) , RDFS.comment , Literal(\"institution\") ))\n",
    "            g.add(( URIRef(eduplace) , URIRef(wdt.P625) , Literal(eduplace_coord) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain Spacy to recognize specific organizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train spacy with new data\n",
    "\n",
    "ner=nlp.get_pipe(\"ner\")\n",
    "TRAIN_DATA = [\n",
    "    (\"Working at Villa I Tatti.\", {\"entities\": [(11, 24, \"ORG\")]})\n",
    "]\n",
    "\n",
    "for _, annotations in TRAIN_DATA:\n",
    "    for ent in annotations.get(\"entities\"):\n",
    "        ner.add_label(ent[2])\n",
    "        \n",
    "pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "\n",
    "# TRAINING THE MODEL\n",
    "with nlp.disable_pipes(*unaffected_pipes):\n",
    "\n",
    "  # Training for 30 iterations\n",
    "  for iteration in range(30):\n",
    "\n",
    "    # shuufling examples  before every iteration\n",
    "    random.shuffle(TRAIN_DATA)\n",
    "    losses = {}\n",
    "    # batch up the examples using spaCy's minibatch\n",
    "    batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n",
    "    for batch in batches:\n",
    "        texts, annotations = zip(*batch)\n",
    "        nlp.update(\n",
    "                    texts,  # batch of texts\n",
    "                    annotations,  # batch of annotations\n",
    "                    drop=0.5,  # dropout - make it harder to memorise data\n",
    "                    losses=losses,\n",
    "                )\n",
    "        print(\"Losses\", losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get places and organizations mentioned in the biographies of historians from ARTchives. Reconcile Named Entities with Wikidata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artchives_places = []\n",
    "for historian in art_historians:\n",
    "    if \"bio\" in historian:\n",
    "        doc = nlp(historian[\"bio\"])\n",
    "        print('\\n' , historian[\"name\"])\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == 'GPE' or ent.label_ == 'ORG':\n",
    "                place = {}\n",
    "                place[\"historian\"] = historian[\"uri\"]\n",
    "                place[\"name\"] = ent.text\n",
    "                place[\"type\"] = ent.label_\n",
    "                ent_text = ent.text[4:] if (ent.text).startswith(\"the\") else ent.text\n",
    "                qid = wikidata_reconciliation(ent_text)\n",
    "                place[\"qid\"] = qid[0] if qid != 'not matched' else \"not found\"\n",
    "                print(place)\n",
    "                artchives_places.append(place)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disambiguate place names that appear also in the name of the historian.\n",
    "\n",
    "### Find coordinates in Wikidata. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_list = []\n",
    "for historian in artchives_places:\n",
    "    historian[\"historian_name\"] =  [hist[\"name\"] for hist in art_historians if hist[\"uri\"] == historian[\"historian\"]][0]\n",
    "    if historian[\"name\"] not in historian[\"historian_name\"] and historian[\"qid\"] != 'not found':\n",
    "        entities_list.append(\"<http://www.wikidata.org/entity/\"+historian[\"qid\"]+\">\")\n",
    "\n",
    "for group_entities in chunks(entities_list, 100):\n",
    "    entities_tbr = \" \".join(list(set(group_entities)))  \n",
    "    wd_coordinates = \"\"\"\n",
    "        PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
    "        SELECT DISTINCT ?place ?place_label ?coord (group_concat(?type_label ; separator=\"; \") as ?label) \n",
    "        WHERE {\n",
    "            VALUES ?place {\"\"\"+entities_tbr+\"\"\"} . \n",
    "            ?place wdt:P625 ?coord.\n",
    "            OPTIONAL {?place rdfs:label ?place_label . FILTER (langMatches(lang(?place_label), \"EN\")) .} \n",
    "            OPTIONAL {?place wdt:P31 ?type . ?type rdfs:label ?type_label . FILTER (langMatches(lang(?type_label), \"EN\"))} \n",
    "            \n",
    "            } \n",
    "            group by ?place ?place_label ?coord ?label\n",
    "        \"\"\"\n",
    "\n",
    "    # set the endpoint \n",
    "    sparql_wd = SPARQLWrapper(wikidata_endpoint)\n",
    "    # set the query\n",
    "    sparql_wd.setQuery(wd_coordinates)\n",
    "    # set the returned format\n",
    "    sparql_wd.setReturnFormat(JSON)\n",
    "    # get the results\n",
    "    results = sparql_wd.query().convert()\n",
    "\n",
    "    loc_list = ['country', 'city', 'village', 'capital', 'state', 'region', 'municipality', 'county', 'frazione', 'comune', 'city-state', 'enclave']\n",
    "\n",
    "    for result in results[\"results\"][\"bindings\"]:\n",
    "        loc = ''\n",
    "        place = result[\"place\"][\"value\"]\n",
    "        place_name = result[\"place_label\"][\"value\"] if \"place_label\" in result else \"no name\"\n",
    "        print(place, place_name)\n",
    "        coord = result[\"coord\"][\"value\"][6:-1].split(\" \")\n",
    "        type_label = result[\"label\"][\"value\"].split(\"; \")[0] if \"label\" in result else \"no name\"\n",
    "        type_label_list = type_label.split(\" \")\n",
    "        check =  any(item in loc_list for item in type_label_list)\n",
    "        if check:\n",
    "            loc = \"geoloc\"\n",
    "        else:\n",
    "            loc = 'institution'\n",
    "\n",
    "        g.add(( URIRef(place) , RDFS.label , Literal(place_name) ))\n",
    "        g.add(( URIRef(place) , wdt.P625 , Literal(coord) ))\n",
    "        g.add(( URIRef(place) , RDFS.comment , Literal(loc) ))     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link new places to historians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for historian in artchives_places:\n",
    "    qid = URIRef(\"http://www.wikidata.org/entity/\"+historian[\"qid\"]) if historian[\"qid\"] != 'not found' else None\n",
    "    if qid and (qid, RDFS.label, None) in g:\n",
    "        g.add(( URIRef(historian[\"historian\"]) , wdt.P7153 , qid )) # P7153 significant place\n",
    "        \n",
    "g.serialize(destination='RQ1.nq', format='nquads') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_RQ1 = []\n",
    "\n",
    "g = rdflib.ConjunctiveGraph()\n",
    "\n",
    "# parse a local RDF file by specifying the format\n",
    "result = g.parse(\"RQ1.nq\", format='nquads')\n",
    "\n",
    "qres = g.query(\n",
    "    \"\"\"\n",
    "    PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
    "    SELECT DISTINCT ?placeqid ?placename ?coords ?placetype ?historianuri ?historianlabel \n",
    "       WHERE {\n",
    "          ?placeqid wdt:P625 ?coords .\n",
    "          OPTIONAL {?placeqid rdfs:label ?placename ; rdfs:comment ?placetype .}\n",
    "          ?historianuri (wdt:P7153 | wdt:P108 | wdt:P69) ?placeqid ; rdfs:label ?historianlabel\n",
    "       }\"\"\")\n",
    "\n",
    "unique_places = list(set([row[0] for row in qres]))\n",
    "\n",
    "for place in unique_places:\n",
    "    place_dict = defaultdict(list)\n",
    "    for row in qres:\n",
    "        if row[0] == place:\n",
    "            place_dict[\"QID\"] = str(row[0])\n",
    "            place_dict[\"label\"] = str(row[1])\n",
    "            place_dict[\"coords\"] = str(row[2])\n",
    "            place_dict[\"type\"] = str(row[3])\n",
    "            place_dict[\"historians\"]. append(( str(row[4]),str(row[5])  ))\n",
    "    place_dict = dict(place_dict)\n",
    "    data_RQ1.append(place_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_RQ1_hist = []\n",
    "unique_historians = list(set(row[4] for row in qres))\n",
    "for hist in unique_historians:\n",
    "    hist_dict = defaultdict(list)\n",
    "    for row in qres:\n",
    "        if row[4] == hist:\n",
    "            hist_dict[\"historian\"] = str(row[4])\n",
    "            hist_dict[\"label\"] = str(row[5])\n",
    "            loc_type = \" (loc)\" if str(row[3]) == \"geoloc\" else \" (inst)\"\n",
    "            place = \"United States (loc)\" if str(row[1]) == \"United States of America\" else str(row[1])+loc_type\n",
    "            hist_dict[\"places\"].append(place)\n",
    "    hist_dict[\"places\"] = list(set(hist_dict[\"places\"]))\n",
    "    hist_dict = dict(hist_dict)\n",
    "    data_RQ1_hist.append(hist_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('RQ1.json', 'w') as f:\n",
    "    json.dump(data_RQ1, f, indent=4)\n",
    "with open('RQ1_hist.json', 'w') as f:\n",
    "    json.dump(data_RQ1_hist, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
